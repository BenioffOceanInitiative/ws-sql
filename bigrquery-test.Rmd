---
title: "bigrquery-test"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Reference: [bigrquery.r-dbi.org](https://bigrquery.r-dbi.org)

## Authorize attempt 1

For non-interactive usage, using:

- [ships4whales@benioff-ocean-initiative.iam.gserviceaccount.com](https://console.cloud.google.com/iam-admin/serviceaccounts/details/114569616080626900590?authuser=3&project=benioff-ocean-initiative)

### sidenote: alternate account

- [whalesafe_table_updater – IAM & Admin – Benioff Ocean Initi… – Google Cloud Platform](https://console.cloud.google.com/iam-admin/serviceaccounts/details/108185723528190775192?authuser=3&project=benioff-ocean-initiative)

- Service account token, per [How to get your own API credentials • gargle](https://gargle.r-lib.org/articles/get-api-credentials.html#service-account-token)
  - From the Developers Console, in the target GCP Project, go to IAM & Admin > Service accounts
  - whalesafe_table_updater > KEYS menu > ADD KEY button
  - Downloaded [benioff-ocean-initiative-b208e85608c2.json](https://drive.google.com/open?id=1ClWXfMMVxufY4-_EPyOj2hHnC4dU3qtS&authuser=ben%40ecoquants.com&usp=drive_fs) into [gfw - Google Drive](https://drive.google.com/drive/u/3/folders/1crBGnOPGiKdWbtOLQhzgdJKA1ztZBzTM) and uploaded into server `/home/admin`

```{r}
library(bigrquery)
library(here)

# ships4whales@benioff-ocean-initiative.iam.gserviceaccount.com
# auth_json = '/home/admin/Benioff Ocean Initiative-454f666d1896.json'
auth_json = '/Volumes/GoogleDrive/My Drive/projects/whalesafe/data/gfw/Benioff Ocean Initiative-454f666d1896.json'
# retrying with new Google Console Service Account API Key after getting Access Denied
# auth_json = '/Volumes/GoogleDrive/.shortcut-targets-by-id/1SHlPwuO32zevQ6W9ZHb-CQLigiXRFeDY/AIS Ship Report Cards/Credentials/benioff-ocean-initiative-27213fdae179.json'
stopifnot(file.exists(auth_json))
bq_auth(path = auth_json)
```


```{r}
(projects <- bq_projects())
prj <- projects[[1]]
bq_project_datasets(prj)
jobs <- try(bq_project_jobs(prj))
jobs[1:3]
```

```{r}
sql <- "SELECT year, month, day, weight_pounds FROM `publicdata.samples.natality`"

tb <- try(bq_project_query(prj, sql))
if (!"try-error" %in% class(tb))
   try(bq_table_download(tb, max_results = 10))
```

## DBI

- [db.rstudio.com/databases/big-query](https://db.rstudio.com/databases/big-query/)

```{r}
library(DBI)

con <- dbConnect(
  bigrquery::bigquery(),
  project = "benioff-ocean-initiative",
  dataset = "whalesafe_v3",
  billing = "benioff-ocean-initiative")
con
```

## Connections

- [RStudio Connections Pane](https://db.rstudio.com/rstudio/connections/)

```{r}
library(connections)
library(bigrquery)

con <- connection_open(
  bigrquery::bigquery(),
  project = "benioff-ocean-initiative",
  dataset = "whalesafe_v3",
  billing = "benioff-ocean-initiative",
  use_legacy_sql = FALSE)
```


Except cannot expand tables to see columns: R Code Execution Error.

eval=F:
```{r, eval=F}
connection_close(con)
```

## Chunks

* [Using SQL in RStudio](https://irene.rbind.io/post/using-sql-in-rstudio/)

```{sql connection=con, eval=F}
SELECT * FROM ais_segments LIMIT 10;
```

```
Error: Cannot query over table 'ais_data' without a filter over column(s) 'timestamp' that can be used for partition elimination [invalidQuery]
In addition: Warning message:
In class(obj) <- c("scalar", class(obj)) :
  Setting class(x) to multiple strings ("scalar", "SQL", ...); result will no longer be an S4 object
Failed to execute SQL chunk
```



```{sql connection=con, eval=F}
SELECT * FROM ais_segments 
WHERE DATE(timestamp) > (CURRENT_DATE() - 7)
LIMIT 10;
```

```
Running job 'benioff-ocean-initiative.job_QNlp_dzhVrQ38KoJgTf8hFxIZ-3f.US' [|]  1s
Complete
Billed: 13.63 MB
Downloading 10 rows in 1 pages.
Error in bq_parse_files(schema_path, page_paths, n = page_info$n_rows,  : 
  Unknown type GEOGRAPHY
In addition: Warning message:
In class(obj) <- c("scalar", class(obj)) :
  Setting class(x) to multiple strings ("scalar", "SQL", ...); result will no longer be an S4 object
Failed to execute SQL chunk
```

```{sql connection=con}
SELECT * FROM whalesafe_v3.INFORMATION_SCHEMA.TABLES ORDER BY table_name;
```

```{sql connection=con}
SELECT
 -- * 
 -- EXCEPT(is_generated, generation_expression, is_stored, is_updatable)
 data_type, column_name, is_partitioning_column
FROM
 whalesafe_v3.INFORMATION_SCHEMA.COLUMNS
WHERE
 table_name="ais_segments"
ORDER BY
 data_type, column_name;
```

```{sql connection=con}
SELECT * EXCEPT(linestring, point)
FROM ais_segments 
WHERE DATE(timestamp) > (CURRENT_DATE() - 7)
ORDER BY timestamp DESC
LIMIT 10;
```


## Parameters

References:

* [Running parameterized queries  |  BigQuery  |  Google Cloud](https://cloud.google.com/bigquery/docs/parameterized-queries)
* [Named parameters  |  BigQuery  |  Google Cloud](https://cloud.google.com/bigquery/docs/samples/bigquery-query-params-named)
* [sql - how to set named parameter on bigquery query - Stack Overflow](https://stackoverflow.com/questions/42628367/how-to-set-named-parameter-on-bigquery-query)
* [Query parameters in the BigQuery UI [35905569] - Visible to Public - Issue Tracker](https://issuetracker.google.com/issues/35905569?pli=1)
* [Run Queries Safely](https://db.rstudio.com/best-practices/run-queries-safely/)
* [Submit query to BigQuery — bq_query • bigrquery](https://bigrquery.r-dbi.org/reference/bq_query.html)
* [BigQuery jobs: perform a job — api-perform • bigrquery](https://bigrquery.r-dbi.org/reference/api-perform.html)
* [Explicitly define query parameters — bq_param • bigrquery](https://bigrquery.r-dbi.org/reference/bq_param.html)
* [S3 classes that reference remote BigQuery datasets, tables and jobs — bq_refs • bigrquery](https://bigrquery.r-dbi.org/reference/bq_refs.html)
* [Dynamic queries with BigQuery from R - Stack Overflow](https://stackoverflow.com/questions/51747814/dynamic-queries-with-bigquery-from-r)

```{r}
tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = "SELECT CURRENT_DATE() as the_date;")
bq_table_download(tb)


sql <- "
  SELECT * EXCEPT(linestring, point)
  FROM ais_segments 
  WHERE DATE(timestamp) > (CURRENT_DATE() - 7)
  ORDER BY timestamp DESC
  LIMIT 10;"
# ds <- bq_dataset("benioff-ocean-initiative", "whalesafe_v3")
tb <- bq_dataset_query(
  x = "benioff-ocean-initiative.whalesafe_v3",
  query = sql,
  billing = "benioff-ocean-initiative")
bq_table_download(tb)


library(dplyr)
library(here)

sql <- readLines(here("gfw/test.sql")) %>% 
  paste(collapse = "\n") 
cat(sql)

# sql <- "SELECT CURRENT_DATE() as the_date;"
tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = sql,
  parameters=list(
    messages_scored_table = "world-fishing-827.pipe_production_v20201001.messages_scored_", 
    research_satellite_timing_table = "world-fishing-827.gfw_research.pipe_v20201001_satellite_timing"))
#bq_table_download(tb)
```

```
Query parameters cannot be used in place of table names at [22:3] [invalidQuery]
```

* [airflow - Is there a way to use dynamic dataset name in bigquery - Stack Overflow](https://stackoverflow.com/questions/60397307/is-there-a-way-to-use-dynamic-dataset-name-in-bigquery)


```{r}
library(glue)

messages_scored_table = "world-fishing-827.pipe_production_v20201001.messages_scored_"
research_satellite_timing_table = "world-fishing-827.gfw_research.pipe_v20201001_satellite_timing"

sql <- readLines(here("gfw/test_glue.sql")) %>% 
  paste(collapse = "\n") %>% 
  glue(
    .open = "{{",
    .close = "}}")
cat(sql)

# sql <- "SELECT CURRENT_DATE() as the_date;"
tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = sql)
#bq_table_download(tb)
```

```
x Not found: Table world-fishing-827:pipe_production_v20201001.messages_scored_ was not found in location US
```

```{r}
library(glue)

messages_scored_table = "world-fishing-827.pipe_production_v20201001.messages_scored_*"
research_satellite_timing_table = "world-fishing-827.gfw_research.pipe_v20201001_satellite_timing"
mdate <- Sys.Date() - 2

sql <- readLines(here("gfw/test_glue2.sql")) %>% 
  paste(collapse = "\n") %>% 
  glue() # cat(sql)

tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = sql)
# bq_table_download(tb)
```


```{r}
library(bigrquery)

# ships4whales@benioff-ocean-initiative.iam.gserviceaccount.com
auth_json = '/Volumes/GoogleDrive/My Drive/projects/whalesafe/data/gfw/Benioff Ocean Initiative-454f666d1896.json'
bq_auth(path = auth_json)

# works
tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = "SELECT * EXCEPT(linestring, point)
    FROM `benioff-ocean-initiative.whalesafe_v3.ais_segments`
    WHERE DATE(timestamp) > (CURRENT_DATE() - 7)
    LIMIT 10;")
# Running job 'benioff-ocean-initiative.job_m8HLodpfodohXd2cdbIdFtA3hxCy.US' [|]  4s
# Complete
# Billed: 22.02 MB

tb <- bq_project_query(
  x = "benioff-ocean-initiative",
  query = "SELECT msgid, timestamp, seg_id, lat, lon FROM
    `world-fishing-827.pipe_production_v20201001.messages_scored_20210518`
    LIMIT 10;")
# Error: Job 'benioff-ocean-initiative.job_yobEApcCD046QaUBfVOdgFf14LPK.US' failed
# x Access Denied: Table world-fishing-827:pipe_production_v20201001.messages_scored_20210518:
#   User does not have permission to query table
#     world-fishing-827:pipe_production_v20201001.messages_scored_20210518. [accessDenied]
```

