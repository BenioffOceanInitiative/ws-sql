---
title: "Dynamic US VSR Scraper: XML to GeoJSON"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## Setup

See [US VSR zone data scraper · Issue #2 · BenioffOceanInitiative/ws-sql](https://github.com/BenioffOceanInitiative/ws-sql/issues/2)

```{r}
# TODO: 
# - injecting into BigQuery
# - setup as a cron job service
# libraries ---
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}
shelf(
  dplyr,
  DT,
  glue,
  here,
  leaflet,
  purrr,
  sf,
  tidyr,
  xml2)

# variables ----
url <- "https://apps-nefsc.fisheries.noaa.gov/cgi-bin/mammalmaps/xmlgenDMA.pl"
xml <- here("data/usa_nefsc_dyn-mgt-areas_raw.xml")
geo <- here("data/usa_nefsc_dyn-mgt-areas.geojson")
```

## Ingest

Setting `eval=FALSE` because empty today `r Sys.Date()`.

```{r eval=F}
# ingest ----
download.file(url, xml)

x <- read_xml(xml) # alternative method of reading 

nodes <- xml_find_all(x, "//dma")

d <- nodes %>% 
  xml_attrs() %>% 
  map_df(~as.list(.)) %>% 
  mutate(
    waypoints = map(id, function(id){ # id = d$id[1]
      w <- nodes %>% 
        xml_find_all(glue("//dma[@id='{id}']/child::node()")) %>% 
        xml_attrs() %>%
        map_df(~as.list(.))
      
      w %>% 
        # duplicate first lat/lon values for creating polygons
        bind_rows(
          w %>% slice(1))} ))

p <- d %>% 
  unnest(waypoints) %>% 
  select(id, lon, lat) %>% 
  group_by(id) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>% 
  summarize(sfc_geometry = st_combine(geometry)) %>% 
  st_cast("POLYGON") %>% 
  left_join(
    d %>% 
      select(-waypoints),
    by = "id")

st_write(p, geo, delete_dsn=T)
```

## Show

```{r}
p <- st_read(geo)

p %>% 
  st_drop_geometry() %>% 
  datatable()

leaflet(p) %>%
  addProviderTiles(providers$Esri.OceanBasemap) %>% 
  addPolygons() 
```


